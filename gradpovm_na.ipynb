{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import sympy as sym\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import cmath as cm\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defininf the states\n",
    "# defininsg coefficeients sybols\n",
    "\n",
    "def Creating_states(coeff, abstract = False):    # coeff list like [a0,a1,b0,b1]\n",
    "    if abstract == True:\n",
    "        a0 = sym.symbols('a0')\n",
    "        a1 = sym.symbols('a1')\n",
    "        b0 = sym.symbols('b0')\n",
    "        b1 = sym.symbols('b1')\n",
    "    else:\n",
    "        a0 = coeff[0]\n",
    "        a1 = coeff[1]\n",
    "        b0 = coeff[2]\n",
    "        b1 = coeff[3]\n",
    "    psi0 = [a0,a1]      # defining states\n",
    "    psi1 = [b0,b1]\n",
    "    return([psi0,psi1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundc(c, digits):\n",
    "    if c.imag == 0:\n",
    "        return round(c.real, digits)\n",
    "    else:\n",
    "        return round(c.real, digits) + round(c.imag, digits) * 1j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the SIC POVM matrices\n",
    "w = torch.exp((2/3)*torch.complex(torch.tensor([0.0]), torch.tensor([1.0]))*m.pi)\n",
    "POVM_vec = (1/(2**.5))*(torch.tensor([[0,1,-1],[-1,0,1],[1,-1,0],[0,w,-w**2],[-1,0,w**2],[1,-w,0],[0,w**2,-w],[-1,0,w],[1,-w**2,0]], dtype=torch.cdouble))  # an array of POVM direction vectors\n",
    "POVM_elts = [(1/3)*torch.outer(torch.conj(POVM_vec[i]),POVM_vec[i]) for i in range(len(POVM_vec))]   # a list of POVM matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_out(w0, f0, f1, a0, a1):\n",
    "\n",
    "    y_pred = torch.empty(9)\n",
    "    \n",
    "    w1 = 1-w0\n",
    "    b0 = torch.sqrt(1 - a0**2)\n",
    "    b1 = torch.sqrt(1 - a1**2)\n",
    "\n",
    "    n2 = torch.tensor([2.0])\n",
    "    \n",
    "    y_pred[0] = (1/6)*(w0*(b0**2)*(1 + a0**2 - 2*torch.sqrt(n2)*torch.cos(f0)*a0*b0) + w1*(b1**2)*(1 + a1**2 - 2*torch.sqrt(n2)*torch.cos(f1)*a1*b1)) #p1\n",
    "    y_pred[1] = (1/6)*(w0*(a0**2)*(1 + b0**2 - 2*torch.sqrt(n2)*torch.cos(f0)*a0*b0) + w1*(a1**2)*(1 + b1**2 - 2*torch.sqrt(n2)*torch.cos(f1)*a1*b1)) #p2\n",
    "    y_pred[2] = (1/6)*(w0*(a0**4 + b0**4 - 2*torch.cos(2*f0)*(a0**2)*(b0**2)) + w1*(a1**4 + b1**4 - 2*torch.cos(2*f1)*(a1**2)*(b1**2))) #p3\n",
    "    y_pred[3] = (1/6)*(w0*(b0**2)*(1 + a0**2 - 2*torch.sqrt(n2)*torch.cos(3*f0 - 2*torch.pi/3)*a0*b0) + w1*(b1**2)*(1 + a1**2 - 2*torch.sqrt(n2)*torch.cos(3*f1 - 2*torch.pi/3)*a1*b1)) #p4\n",
    "    y_pred[4] = (1/6)*(w0*(a0**2)*(1 + b0**2 - 2*torch.sqrt(n2)*torch.cos(f0 - 4*torch.pi/3)*a0*b0) + w1*(a1**2)*(1 + b1**2 - 2*torch.sqrt(n2)*torch.cos(f1 - 4*torch.pi/3)*a1*b1)) #p5\n",
    "    y_pred[5] = (1/6)*(w0*(a0**4 + b0**4 - 2*torch.cos(2*f0 + 2*torch.pi/3)*(a0**2)*(b0**2)) + w1*(a1**4 + b1**4 - 2*torch.cos(2*f1 + 2*torch.pi/3)*(a1**2)*(b1**2))) #p6\n",
    "    y_pred[6] = (1/6)*(w0*(b0**2)*(1 + a0**2 - 2*torch.sqrt(n2)*torch.cos(3*f0 - 2*torch.pi/3)*a0*b0) + w1*(b1**2)*(1 + a1**2 - 2*torch.sqrt(n2)*torch.cos(3*f1 - 2*torch.pi/3)*a1*b1)) #p7\n",
    "    y_pred[7] = (1/6)*(w0*(a0**2)*(1 + b0**2 - 2*torch.sqrt(n2)*torch.cos(f0 - 2*torch.pi/3)*a0*b0) + w1*(a1**2)*(1 + b1**2 - 2*torch.sqrt(n2)*torch.cos(f1 - 2*torch.pi/3)*a1*b1)) #p8\n",
    "    y_pred[8] = (1/6)*(w0*(a0**4 + b0**4 - 2*torch.cos(2*f0 - 4*torch.pi/3)*(a0**2)*(b0**2)) + w1*(a1**4 + b1**4 - 2*torch.cos(2*f1 - 4*torch.pi/3)*(a1**2)*(b1**2))) #p9\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floss(ny, w0, f0, f1, a0,  a1):\n",
    "    y_pred = torch_out(w0, f0, f1, a0, a1)\n",
    "    log_likelyhood = -torch.sum(ny * torch.log(y_pred))\n",
    "    return log_likelyhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(lr, i):\n",
    "    return lr/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fid(learning_rate,N, coeff, priors, POVM_elts):\n",
    "    #locals().clear()\n",
    "    initial_states = Creating_states(abstract=False, coeff = coeff)     # Creating the two states with these coefficients\n",
    "    psi0 = initial_states[0]\n",
    "    psi1 = initial_states[1]    # created the states to be discriminated\n",
    "\n",
    "    psi0sq = []\n",
    "    psi1sq = []\n",
    "    [[psi0sq.append(i*j) for i in psi0] for j in psi0]\n",
    "    [[psi1sq.append(i*j) for i in psi1] for j in psi1]   \n",
    "    \n",
    "    psi0sq = torch.tensor(psi0sq, dtype=torch.float32)\n",
    "    psi1sq = torch.tensor(psi1sq, dtype=torch.float32)\n",
    "\n",
    "    psi0psi0 = [psi0sq[0], torch.sqrt(psi0sq[1]**2+psi0sq[2]**2), psi0sq[3]]\n",
    "    psi1psi1 = [psi1sq[0], torch.sqrt(psi1sq[1]**2+psi1sq[2]**2), psi1sq[3]]  # creating square states\n",
    "\n",
    "    vec_psi0psi0 = torch.tensor(psi0psi0, dtype=torch.cdouble)\n",
    "    vec_psi1psi1 = torch.tensor(psi1psi1, dtype=torch.cdouble)\n",
    "\n",
    "    rho = priors[0]*torch.outer(vec_psi0psi0, torch.conj(vec_psi0psi0)) + priors[1]*torch.outer(vec_psi1psi1, torch.conj(vec_psi1psi1))  # theoretical density matrix with priors 1/2 each.\n",
    "\n",
    "    prob_vec =  [torch.trace(torch.matmul(POVM_elts[i],rho)) for i in range(9)] \n",
    "    prob_vec = [i.real for i in prob_vec if abs(i.imag) < .01]          # cleaned up theoretical prob vector\n",
    "\n",
    "    POVM_dir_symbols = ['d1','d2','d3','d4','d5','d6','d7','d8','d9']      # symbols to indicate collapsed direction\n",
    "    #prob distribution is simply the corresponding elements of the prob_vec\n",
    "    collapse_dir_vec = rand.choices(POVM_dir_symbols, weights=prob_vec, k = N)   # choosing collapse directions with weights for N trials\n",
    "\n",
    "    nj_vec = [collapse_dir_vec.count(f'd{i+1}') for i in range(9)]\n",
    "\n",
    "    ##################################### GRaDIENT DESCENT ##############################################\n",
    "    \n",
    "    nj_vec = torch.tensor(nj_vec, dtype = torch.int64)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    w0 = torch.empty(1).uniform_(0, 1).requires_grad_()\n",
    "    f0 = torch.empty(1).uniform_(0, 2 * torch.pi).requires_grad_() #torch.tensor([0.0], requires_grad=True)\n",
    "    f1 = torch.empty(1).uniform_(0, 2 * torch.pi).requires_grad_() #torch.tensor([0.0], requires_grad=True)\n",
    "    a0 = torch.empty(1).uniform_(0, 1).requires_grad_()\n",
    "    a1 = torch.empty(1).uniform_(0, 1).requires_grad_()\n",
    "\n",
    "    '''w0 = torch.tensor([.55], requires_grad=True)\n",
    "    f0 = torch.tensor([0.1], requires_grad=True)\n",
    "    f1 = torch.tensor([0.1], requires_grad=True)\n",
    "    a0 = torch.tensor([0.1], requires_grad=True)\n",
    "    a1 = torch.tensor([0.9], requires_grad=True)'''\n",
    "\n",
    "    # Set learning rate\n",
    "    #learning_rate = 0.001 + 1**(N/10000) #0.01\n",
    "    \n",
    "    #print(f'exp learning rate: {N/10000}')\n",
    "    #print(f'learning rate: {learning_rate}')\n",
    "\n",
    "    # Create a list to store the loss at each epoch\n",
    "    loss_history = []\n",
    "    w0_grad = []\n",
    "    f0_grad = []\n",
    "    f1_grad = []\n",
    "    a0_grad = []\n",
    "    a1_grad = []\n",
    "\n",
    "\n",
    "    # Train for N epochs\n",
    "    for i in range(100):\n",
    "        \n",
    "        \n",
    "        # Forward pass:\n",
    "\n",
    "        loss = floss(nj_vec, w0, f0, f1, a0, a1)\n",
    "        # append the current loss to the history\n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        w0_grad.append(w0.grad.item())\n",
    "        f0_grad.append(f0.grad.item())\n",
    "        f1_grad.append(f1.grad.item())\n",
    "        a0_grad.append(a0.grad.item())\n",
    "        a1_grad.append(a1.grad.item())\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        with torch.no_grad():\n",
    "            w0 -= learning_rate * w0.grad\n",
    "            f0 -= learning_rate * f0.grad\n",
    "            f1 -= learning_rate * f1.grad\n",
    "            a0 -= learning_rate * a0.grad\n",
    "            a1 -= learning_rate * a1.grad\n",
    "\n",
    "        # Zero gradients for the next iteration\n",
    "        w0.grad.zero_()\n",
    "        f0.grad.zero_()\n",
    "        f1.grad.zero_()\n",
    "        a0.grad.zero_()\n",
    "        a1.grad.zero_()\n",
    "\n",
    "    # after the training loop\n",
    "    plt.figure()\n",
    "    plt.plot(loss_history, label='Loss')\n",
    "    plt.plot(w0_grad, label='w0_grad')\n",
    "    plt.plot(f0_grad, label='f0_grad')\n",
    "    plt.plot(f1_grad, label='f1_grad')\n",
    "    plt.plot(a0_grad, label='a0_grad')\n",
    "    plt.plot(a1_grad, label='a1_grad')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()                                   \n",
    "    \n",
    "    w1 = 1 - w0\n",
    "    b0 = torch.sqrt(1 - a0**2)\n",
    "    b1 = torch.sqrt(1 - a1**2)\n",
    "    ph0 = torch.cos(f0) + 1j*torch.sin(f0)\n",
    "    ph1 = torch.cos(f1) + 1j*torch.sin(f1)\n",
    "    \n",
    "    psi0n = torch.tensor([a0, ph0*a1], dtype=torch.cdouble)\n",
    "    psi1n = torch.tensor([b0, ph1*b1], dtype=torch.cdouble)\n",
    "    psi0t = torch.tensor(initial_states[0], dtype=torch.cdouble)\n",
    "    psi1t = torch.tensor(initial_states[1], dtype=torch.cdouble)\n",
    "\n",
    "    fid0 = torch.abs(torch.dot(psi0n, torch.conj(psi0t)))**2\n",
    "    fid1 = torch.abs(torch.dot(psi1n, torch.conj(psi1t)))**2\n",
    "\n",
    "    fid = [fid0.item(), fid1.item()]\n",
    "    p = [w0.item(), w1.item()]\n",
    "\n",
    "    return([fid,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''coeff = [0**.5,1**.5,1**.5,0**.5] \n",
    "priors = [.5,.5]\n",
    "N = 1000\n",
    "\n",
    "#learning_rate = 0.001\n",
    "\n",
    "initial_states = Creating_states(abstract=False, coeff = coeff)     # Creating the two states with these coefficients\n",
    "psi0 = initial_states[0]\n",
    "psi1 = initial_states[1]    # created the states to be discriminated\n",
    "\n",
    "psi0sq = []\n",
    "psi1sq = []\n",
    "[[psi0sq.append(i*j) for i in psi0] for j in psi0]\n",
    "[[psi1sq.append(i*j) for i in psi1] for j in psi1]   \n",
    "\n",
    "psi0sq = torch.tensor(psi0sq, dtype=torch.float32)\n",
    "psi1sq = torch.tensor(psi1sq, dtype=torch.float32)\n",
    "\n",
    "psi0psi0 = [psi0sq[0], torch.sqrt(psi0sq[1]**2+psi0sq[2]**2), psi0sq[3]]\n",
    "psi1psi1 = [psi1sq[0], torch.sqrt(psi1sq[1]**2+psi1sq[2]**2), psi1sq[3]]  # creating square states\n",
    "\n",
    "vec_psi0psi0 = torch.tensor(psi0psi0, dtype=torch.cdouble)\n",
    "vec_psi1psi1 = torch.tensor(psi1psi1, dtype=torch.cdouble)\n",
    "\n",
    "rho = priors[0]*torch.outer(vec_psi0psi0, torch.conj(vec_psi0psi0)) + priors[1]*torch.outer(vec_psi1psi1, torch.conj(vec_psi1psi1))  # theoretical density matrix with priors 1/2 each.\n",
    "\n",
    "prob_vec =  [torch.trace(torch.matmul(POVM_elts[i],rho)) for i in range(9)] \n",
    "prob_vec = [i.real for i in prob_vec if abs(i.imag) < .01]          # cleaned up theoretical prob vector\n",
    "\n",
    "POVM_dir_symbols = ['d1','d2','d3','d4','d5','d6','d7','d8','d9']      # symbols to indicate collapsed direction\n",
    "#prob distribution is simply the corresponding elements of the prob_vec\n",
    "collapse_dir_vec = rand.choices(POVM_dir_symbols, weights=prob_vec, k = N)   # choosing collapse directions with weights for N trials\n",
    "\n",
    "nj_vec = [collapse_dir_vec.count(f'd{i+1}') for i in range(9)]\n",
    "\n",
    "##################################### GRaDIENT DESCENT ##############################################\n",
    "    \n",
    "nj_vec = torch.tensor(nj_vec, dtype = torch.int64)\n",
    "\n",
    "# Define your fixed values for f0, f1, t0, t1\n",
    "f0_fixed = torch.tensor([torch.pi / 2])\n",
    "f1_fixed = torch.tensor([torch.pi / 4])\n",
    "t0_fixed = torch.tensor([torch.pi / 4])\n",
    "t1_fixed = torch.tensor([torch.pi / 4])\n",
    "\n",
    "# Define the range of values for a\n",
    "a_values = torch.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "# Compute torch_out for each value of a\n",
    "torch_out_values = [torch_out(a,f0_fixed,f1_fixed,t0_fixed,t1_fixed)[0] for a in a_values]\n",
    "floss_values = [floss(nj_vec, a, f0_fixed, f1_fixed, t0_fixed, t1_fixed) for a in a_values]\n",
    "#torch_out_values = [grad_fid(learning_rate, N, coeff, priors, POVM_elts)[0] for a in a_values]\n",
    "\n",
    "# Plot the dependence of torch_out on a\n",
    "plt.plot(a_values, floss_values)\n",
    "#plt.plot(a_values.detach().numpy(), torch_out_values)\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('floss')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = [0**.5,1**.5,1**.5,0**.5] \n",
    "priors = [.5,.5]\n",
    "trials = [100*(i+1) for i in range(100)]\n",
    "#trials = [10000]\n",
    "p0trials  = [priors[0] for i in trials] \n",
    "p1trials  = [priors[1] for i in trials]\n",
    "\n",
    "lr0 = 0.001\n",
    "\n",
    "initial_states = Creating_states(abstract=False, coeff = coeff)     # Creating the two states with these coefficients\n",
    "psi0 = initial_states[0]\n",
    "psi1 = initial_states[1]\n",
    "\n",
    "output_f = []\n",
    "output_p = []\n",
    "\n",
    "pfid  = [1 for i in trials]    # perfect fidelity of 1\n",
    "for i,t in tqdm(enumerate(trials)):\n",
    "    #print(f'i_N: {i}')\n",
    "    lr = update_lr(lr0,i)\n",
    "    #print(f'learning rate: {lr}')\n",
    "    output_f.append(grad_fid(lr, t,coeff,priors,POVM_elts)[0])\n",
    "    output_p.append(grad_fid(lr, t,coeff,priors,POVM_elts)[1])\n",
    "\n",
    "#output_f = [grad_fid(lr0, i, coeff, priors, POVM_elts)[0] for i in tqdm(trials)]\n",
    "#output_p = [grad_fid(lr0, i, coeff, priors, POVM_elts)[1] for i in tqdm(trials)]\n",
    "\n",
    "outputf = list(map(list, zip(*output_f)))\n",
    "outputp = list(map(list, zip(*output_p)))\n",
    "\n",
    "#print(outputf)\n",
    "#print(outputp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_a, (ax1a, ax2a) = plt.subplots(1,2, figsize=(10,4), sharey=True)\n",
    "ax1a.plot(trials, outputf[0], label=r'$F_0=|\\langle \\psi_0|\\psi_0^{\\,n}\\rangle|^2$')\n",
    "ax1a.plot(trials , pfid, \"--\", label=r'$F=1$')\n",
    "ax1a.set_ylim(0,1)\n",
    "ax1a.set_xlabel(' N (trials)')\n",
    "ax1a.set_ylabel(r'Fidelity $F$')\n",
    "ax1a.set_title(r'$|\\psi_0\\rangle ={}|0\\rangle+{}|1\\rangle $, $p_0={}$'.format(roundc(coeff[0],1), roundc(coeff[1],1), priors[0]))\n",
    "ax1a.legend(loc='best')\n",
    "ax2a.plot(trials, outputf[1], label=r'$F_1=|\\langle \\psi_1|\\psi_1^{\\,n}\\rangle|^2$')\n",
    "ax2a.plot(trials , pfid, \"--\", label=r'$F=1$')\n",
    "ax2a.set_ylim(0,1)\n",
    "ax2a.set_xlabel(' N (trials)')\n",
    "ax2a.set_ylabel(r'Fidelity $F$')\n",
    "ax2a.set_title(r'$|\\psi_1\\rangle ={}|0\\rangle+{}|1\\rangle $, $p_1={}$'.format(roundc(coeff[2],1), roundc(coeff[3],1), priors[1]))\n",
    "ax2a.legend(loc='best')\n",
    "\n",
    " \n",
    "fig_c, (ax1c, ax2c) = plt.subplots(1,2, figsize=(10,4), sharey=True)\n",
    "ax1c.plot(trials, outputp[0], label=r'$p_{0}$ numerical')\n",
    "ax1c.plot(trials, p0trials, '--', label=r'$p_{0}$ theoretical')\n",
    "ax2c.plot(trials, outputp[1], label=r'$p_{1}$ numerical')\n",
    "ax2c.plot(trials, p1trials, '--', label=r'$p_{1}$ theoretical')\n",
    "ax1c.set_ylim(0,1)\n",
    "ax2c.set_ylim(0,1)\n",
    "ax1c.set_xlabel(' N (trials)')\n",
    "ax2c.set_xlabel(' N (trials)')\n",
    "ax1c.set_ylabel(r'Probability')\n",
    "ax2c.set_ylabel(r'Probability')\n",
    "ax1c.legend(loc='best')\n",
    "ax2c.legend(loc='best')\n",
    "ax1c.set_title(r'$|\\psi_0\\rangle ={}|0\\rangle+{}|1\\rangle $, $p_0={}$'.format(roundc(coeff[0],1), roundc(coeff[1],1), priors[0]))\n",
    "ax2c.set_title(r'$|\\psi_1\\rangle ={}|0\\rangle+{}|1\\rangle $, $p_1={}$'.format(roundc(coeff[2],1), roundc(coeff[3],1), priors[1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import sympy as sym\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import cmath as cm\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import itertools\n",
    "import cmath as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defininf the states\n",
    "# defininsg coefficeients sybols\n",
    "\n",
    "def Creating_states(coeff, abstract = False):    # coeff list like [a0,a1,b0,b1]\n",
    "    if abstract == True:\n",
    "        a0 = sym.symbols('a0')\n",
    "        a1 = sym.symbols('a1')\n",
    "        b0 = sym.symbols('b0')\n",
    "        b1 = sym.symbols('b1')\n",
    "    else:\n",
    "        a0 = coeff[0]\n",
    "        a1 = coeff[1]\n",
    "        b0 = coeff[2]\n",
    "        b1 = coeff[3]\n",
    "    psi0 = [a0,a1]      # defining states\n",
    "    psi1 = [b0,b1]\n",
    "    return([psi0,psi1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundc(c, digits):\n",
    "    if c.imag == 0:\n",
    "        return round(c.real, digits)\n",
    "    else:\n",
    "        return round(c.real, digits) + round(c.imag, digits) * 1j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(lr, i):\n",
    "    return lr/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnj_vec(N, coeff, priors):\n",
    "\n",
    "    # creating the SIC POVM matrices\n",
    "    w = torch.exp((2/3)*torch.complex(torch.tensor([0.0]), torch.tensor([1.0]))*m.pi)\n",
    "    POVM_vec = (1/(2**.5))*(torch.tensor([[0,1,-1],[-1,0,1],[1,-1,0],[0,w,-w**2],[-1,0,w**2],[1,-w,0],[0,w**2,-w],[-1,0,w],[1,-w**2,0]], dtype=torch.cdouble))  # an array of POVM direction vectors\n",
    "    POVM_elts = [(1/3)*torch.outer(torch.conj(POVM_vec[i]),POVM_vec[i]) for i in range(len(POVM_vec))]   # a list of POVM matrix\n",
    "\n",
    "    initial_states = Creating_states(coeff = coeff, abstract=False)     # Creating the two states with these coefficients\n",
    "    psi0 = initial_states[0]\n",
    "    psi1 = initial_states[1]    # created the states to be discriminated\n",
    "    \n",
    "    psi0sq = []\n",
    "    psi1sq = []\n",
    "    [[psi0sq.append(i*j) for i in psi0] for j in psi0]\n",
    "    [[psi1sq.append(i*j) for i in psi1] for j in psi1]   \n",
    "    psi0psi0 = [psi0sq[0], np.sqrt(psi0sq[1]**2+psi0sq[2]**2), psi0sq[3]]    \n",
    "    psi1psi1 = [psi1sq[0], np.sqrt(psi1sq[1]**(2)+psi1sq[2]**(2)), psi1sq[3]]    # creating square states\n",
    "    \n",
    "    vec_psi0psi0 = np.array(psi0psi0)     \n",
    "    vec_psi1psi1 = np.array(psi1psi1)\n",
    "    rho = priors[0]*np.outer(vec_psi0psi0, vec_psi0psi0)+ priors[1]*np.outer(vec_psi1psi1, vec_psi1psi1)     # theoretical density matrix with priors 1/2 each.\n",
    "\n",
    "    prob_vec =  [np.trace(np.dot(POVM_elts[i],rho)) for i in range(9)] \n",
    "    prob_vec = [i.real for i in prob_vec if abs(i.imag) < .01]          # cleaned up theoretical prob vector\n",
    "\n",
    "    POVM_dir_symbols = ['d1','d2','d3','d4','d5','d6','d7','d8','d9']      # symbols to indicate collapsed direction\n",
    "    #prob distribution is simply the corresponding elements of the prob_vec\n",
    "    collapse_dir_vec = rand.choices(POVM_dir_symbols, weights=prob_vec, k = N)   # choosing collapse directions with weights for N trials\n",
    "\n",
    "    nj_vec = [collapse_dir_vec.count(f'd{i+1}') for i in range(9)]\n",
    "\n",
    "    return nj_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_out(a, f0, f1, t0, t1):\n",
    "\n",
    "    y_pred = torch.empty(9)\n",
    "    \n",
    "    w0 = (torch.cos(a))**2\n",
    "    w1 = (torch.sin(a))**2\n",
    "    a0 = torch.cos(t0)\n",
    "    b0 = torch.sin(t0)\n",
    "    a1 = torch.cos(t1)\n",
    "    b1 = torch.sin(t1)\n",
    "\n",
    "    n2 = torch.tensor([2.0])\n",
    "    \n",
    "    y_pred[0] = (1/6)*(w0*(b0**2)*(1 + a0**2 - 2*torch.sqrt(n2)*torch.cos(f0)*a0*b0) + w1*(b1**2)*(1 + a1**2 - 2*torch.sqrt(n2)*torch.cos(f1)*a1*b1)) #p1\n",
    "    y_pred[1] = (1/6)*(w0*(a0**2)*(1 + b0**2 - 2*torch.sqrt(n2)*torch.cos(f0)*a0*b0) + w1*(a1**2)*(1 + b1**2 - 2*torch.sqrt(n2)*torch.cos(f1)*a1*b1)) #p2\n",
    "    y_pred[2] = (1/6)*(w0*(a0**4 + b0**4 - 2*torch.cos(2*f0)*(a0**2)*(b0**2)) + w1*(a1**4 + b1**4 - 2*torch.cos(2*f1)*(a1**2)*(b1**2))) #p3\n",
    "    y_pred[3] = (1/6)*(w0*(b0**2)*(1 + a0**2 - 2*torch.sqrt(n2)*torch.cos(f0 + 2*torch.pi/3)*a0*b0) + w1*(b1**2)*(1 + a1**2 - 2*torch.sqrt(n2)*torch.cos(f1 + 2*torch.pi/3)*a1*b1)) #p4\n",
    "    y_pred[4] = (1/6)*(w0*(a0**2)*(1 + b0**2 - 2*torch.sqrt(n2)*torch.cos(f0 - 4*torch.pi/3)*a0*b0) + w1*(a1**2)*(1 + b1**2 - 2*torch.sqrt(n2)*torch.cos(f1 + 2*torch.pi/3)*a1*b1)) #p5\n",
    "    y_pred[5] = (1/6)*(w0*(a0**4 + b0**4 - 2*torch.cos(2*f0 - 2*torch.pi/3)*(a0**2)*(b0**2)) + w1*(a1**4 + b1**4 - 2*torch.cos(2*f1 - 2*torch.pi/3)*(a1**2)*(b1**2))) #p6\n",
    "    y_pred[6] = (1/6)*(w0*(b0**2)*(1 + a0**2 - 2*torch.sqrt(n2)*torch.cos(f0 - 2*torch.pi/3)*a0*b0) + w1*(b1**2)*(1 + a1**2 - 2*torch.sqrt(n2)*torch.cos(f1 - 2*torch.pi/3)*a1*b1)) #p7\n",
    "    y_pred[7] = (1/6)*(w0*(a0**2)*(1 + b0**2 - 2*torch.sqrt(n2)*torch.cos(f0 - 2*torch.pi/3)*a0*b0) + w1*(a1**2)*(1 + b1**2 - 2*torch.sqrt(n2)*torch.cos(f1 - 2*torch.pi/3)*a1*b1)) #p8\n",
    "    y_pred[8] = (1/6)*(w0*(a0**4 + b0**4 - 2*torch.cos(2*f0 - 4*torch.pi/3)*(a0**2)*(b0**2)) + w1*(a1**4 + b1**4 - 2*torch.cos(2*f1 + 2*torch.pi/3)*(a1**2)*(b1**2))) #p9\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floss(ny, a, f0, f1, t0,  t1):\n",
    "    y_pred = torch_out(a, f0, f1, t0, t1)\n",
    "    total_ny = torch.sum(ny)\n",
    "    log_likelyhood = -(1/total_ny)*torch.sum(ny * torch.log(y_pred))\n",
    "    return log_likelyhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss vs alpha\n",
    "\n",
    "coeff = [0**.5,1**.5,1**.5,0**.5] \n",
    "priors = [.5,.5]\n",
    "N = 1000\n",
    "\n",
    "nj_vec = fnj_vec(N, coeff, priors)\n",
    "    \n",
    "nj_vec = torch.tensor(nj_vec, dtype = torch.int64)\n",
    "\n",
    "# Define your fixed values for f0, f1, t0, t1\n",
    "f0_fixed = torch.tensor([torch.pi / 2])\n",
    "f1_fixed = torch.tensor([torch.pi / 4])\n",
    "t0_fixed = torch.tensor([torch.pi / 4])\n",
    "t1_fixed = torch.tensor([torch.pi / 2])\n",
    "\n",
    "# Define the range of values for a\n",
    "a_values = torch.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "# Compute torch_out for each value of a\n",
    "torch_out_values = [torch_out(a,f0_fixed,f1_fixed,t0_fixed,t1_fixed) for a in a_values]\n",
    "floss_values = [floss(nj_vec, a, f0_fixed, f1_fixed, t0_fixed, t1_fixed) for a in a_values]\n",
    "\n",
    "# Plot the dependence of torch_out on a\n",
    "plt.plot(a_values, floss_values)\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('floss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss vs f0\n",
    "\n",
    "coeff = [0**.5,1**.5,1**.5,0**.5] \n",
    "priors = [.5,.5]\n",
    "N = 1000\n",
    "\n",
    "nj_vec = fnj_vec(N, coeff, priors)\n",
    "    \n",
    "nj_vec = torch.tensor(nj_vec, dtype = torch.int64)\n",
    "\n",
    "# Define your fixed values for a, f1, t0, t1\n",
    "a_fixed = torch.tensor([torch.pi / 4])\n",
    "f1_fixed = torch.tensor([torch.pi / 4])\n",
    "t0_fixed = torch.tensor([torch.pi / 4])\n",
    "t1_fixed = torch.tensor([torch.pi / 4])\n",
    "\n",
    "# Define the range of values for f0\n",
    "f0_values = torch.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "# Compute torch_out for each value of f0\n",
    "torch_out_values = [torch_out(a_fixed, f0, f1_fixed, t0_fixed, t1_fixed) for f0 in f0_values]\n",
    "floss_values = [floss(nj_vec, a_fixed, f0, f1_fixed, t0_fixed, t1_fixed) for f0 in f0_values]\n",
    "\n",
    "# Plot the dependence of torch_out on f0\n",
    "plt.plot(a_values, floss_values)\n",
    "plt.xlabel('f0')\n",
    "plt.ylabel('floss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss vs t0\n",
    "\n",
    "coeff = [0**.5,1**.5,1**.5,0**.5] \n",
    "priors = [.5,.5]\n",
    "N = 1000\n",
    "\n",
    "nj_vec = fnj_vec(N, coeff, priors)\n",
    "    \n",
    "nj_vec = torch.tensor(nj_vec, dtype = torch.int64)\n",
    "\n",
    "# Define your fixed values for a, f0, f1, t1\n",
    "a_fixed = torch.tensor([torch.pi / 4])\n",
    "f0_fixed = torch.tensor([torch.pi / 4])\n",
    "f1_fixed = torch.tensor([torch.pi / 4])\n",
    "t1_fixed = torch.tensor([torch.pi / 4])\n",
    "\n",
    "# Define the range of values for t0\n",
    "t0_values = torch.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "# Compute torch_out for each value of t0\n",
    "torch_out_values = [torch_out(a_fixed, f0_fixed, f1_fixed, t0, t1_fixed) for t0 in t0_values]\n",
    "floss_values = [floss(nj_vec, a_fixed, f0_fixed, f1_fixed, t0, t1_fixed) for t0 in t0_values]\n",
    "\n",
    "# Plot the dependence of torch_out on t0\n",
    "plt.plot(a_values, floss_values)\n",
    "plt.xlabel('t0')\n",
    "plt.ylabel('floss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fid(learning_rate, N, coeff, priors):\n",
    "    \n",
    "    nj_vec = fnj_vec(N, coeff, priors)\n",
    "\n",
    "    ##################################### GRaDIENT DESCENT ##############################################\n",
    "    \n",
    "    nj_vec = torch.tensor(nj_vec, dtype = torch.int64)\n",
    "    \n",
    "    # Initialize parameters\n",
    "\n",
    "    # Define the intervals\n",
    "    #I_a = [0, np.pi]\n",
    "    #I_f0 = [0, 2*np.pi]\n",
    "    #I_f1 = [0, 2*np.pi]\n",
    "    #I_t0 = [0, 2*np.pi]\n",
    "    #I_t1 = [0, 2*np.pi]\n",
    "\n",
    "    # Divide the intervals and generate the values\n",
    "    #a_values = np.linspace(*I_a, 3)[1:-1]\n",
    "    a_values = [np.pi/4]\n",
    "    #f0_values = np.linspace(*I_f0, 6)[1:-1]\n",
    "    f0_values = np.random.uniform(0.0, np.pi/2, 3).tolist()\n",
    "    #f1_values = np.linspace(*I_f1, 6)[1:-1]\n",
    "    f1_values = np.random.uniform(0.0, np.pi/2, 3).tolist()\n",
    "    #t0_values = np.linspace(*I_t0, 6)[1:-1]\n",
    "    t0_values = np.random.uniform(0.0, np.pi/2, 3).tolist()\n",
    "    #t1_values = np.linspace(*I_t1, 6)[1:-1]\n",
    "    t1_values = np.random.uniform(0.0, np.pi/2, 3).tolist()\n",
    "\n",
    "    # Generate all possible combinations of the values\n",
    "    #initial_conditions = [[torch.tensor([val], requires_grad=True) for val in combination] \n",
    "                        #for combination in itertools.product(a_values, f0_values, f1_values, t0_values, t1_values)]\n",
    "\n",
    "    initial_conditions = [(torch.tensor([i], requires_grad=True), torch.tensor([j], requires_grad=True),torch.tensor([k], requires_grad=True),torch.tensor([l], requires_grad=True),torch.tensor([m], requires_grad=True)) for i in a_values for j in f0_values for k in f1_values for l in t0_values for m in t1_values]\n",
    "\n",
    "\n",
    "    #print(f'initial conditions: {initial_conditions}')\n",
    "\n",
    "    '''# Define the intervals\n",
    "    I_a = [0, torch.pi]\n",
    "    I_f0 = [0, 2*torch.pi]\n",
    "    I_f1 = [0, 2*torch.pi]\n",
    "    I_t0 = [0, 2*torch.pi]\n",
    "    I_t1 = [0, 2*torch.pi]\n",
    "\n",
    "    # Generate 10 different initial conditions\n",
    "    initial_conditions = []\n",
    "    for _ in range(10):\n",
    "        a = torch.tensor([rand.uniform(*I_a)], requires_grad=True)\n",
    "        f0 = torch.tensor([rand.uniform(*I_f0)], requires_grad=True)\n",
    "        f1 = torch.tensor([rand.uniform(*I_f1)], requires_grad=True)\n",
    "        t0 = torch.tensor([rand.uniform(*I_t0)], requires_grad=True)\n",
    "        t1 = torch.tensor([rand.uniform(*I_t1)], requires_grad=True)\n",
    "        initial_conditions.append([a, f0, f1, t0, t1])'''\n",
    "\n",
    "    '''initial_conditions = [\n",
    "    [torch.tensor([val], requires_grad=True) for val in [torch.pi / 4, 0.0, 0.0, torch.pi / 2, 0.0]],\n",
    "    #[torch.tensor([val], requires_grad=True) for val in [torch.pi / 2, 0.0, 0.0, torch.pi / 2, 0.0]],\n",
    "    # Add more initial conditions here\n",
    "    ]'''\n",
    "\n",
    "    '''# Define your initial conditions\n",
    "    values = [0.0, torch.pi / 4, torch.pi / 2, torch.pi]\n",
    "\n",
    "    # Generate all possible 5-dimensional combinations of the values\n",
    "    initial_conditions = [[torch.tensor([val], requires_grad=True) for val in tqdm(combination)] for combination in itertools.product(values, repeat=5)]\n",
    "    ip = [torch.tensor([torch.pi / 4], requires_grad=True),torch.tensor([0.0], requires_grad=True),torch.tensor([0.0], requires_grad=True),torch.tensor([torch.pi / 2], requires_grad=True),torch.tensor([0.0], requires_grad=True)]\n",
    "    a = ip[0]#torch.tensor([torch.pi / 4], requires_grad=True) #in_par[0]\n",
    "    f0 = ip[1]#torch.tensor([0.0], requires_grad=True) #in_par[1]\n",
    "    f1 = ip[2]#torch.tensor([0.0], requires_grad=True) #in_par[2]\n",
    "    t0 = ip[3]#torch.tensor([torch.pi / 2], requires_grad=True)  #in_par[3]\n",
    "    t1 = ip[4]#torch.tensor([0.0], requires_grad=True) #in_par[4]'''\n",
    "\n",
    "    '''a = torch.empty(1).uniform_(0, torch.pi).requires_grad_()\n",
    "    f0 = torch.empty(1).uniform_(0, 2 * torch.pi).requires_grad_()\n",
    "    f1 = torch.empty(1).uniform_(0, 2 * torch.pi).requires_grad_()\n",
    "    t0 = torch.empty(1).uniform_(0, 2 * torch.pi).requires_grad_()\n",
    "    t1 = torch.empty(1).uniform_(0, 2 * torch.pi).requires_grad_()'''\n",
    "\n",
    "    # Set learning rate\n",
    "    #learning_rate = 0.001 + 1**(N/10000) #0.01\n",
    "    \n",
    "    #print(f'exp learning rate: {N/10000}')\n",
    "    #print(f'learning rate: {learning_rate}')\n",
    "\n",
    "\n",
    "    # Run gradient descent for each initial condition\n",
    "    solutions = []\n",
    "    for ip in tqdm(initial_conditions):\n",
    "\n",
    "        # Create a list to store the loss at each epoch\n",
    "        loss_history = []\n",
    "        a_grad = []\n",
    "        f0_grad = []\n",
    "        f1_grad = []\n",
    "        t0_grad = []\n",
    "        t1_grad = []\n",
    "\n",
    "        # Set the initial condition\n",
    "        a, f0, f1, t0, t1 = ip\n",
    "        # Train for N epochs\n",
    "        for i in range(200):\n",
    "            \n",
    "            # Forward pass:\n",
    "            loss = floss(nj_vec, a, f0, f1, t0, t1)\n",
    "            # append the current loss to the history\n",
    "            loss_history.append(loss.item())\n",
    "            \n",
    "\n",
    "            # Backward pass: compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            a_grad.append(a.grad.item())\n",
    "            #a_grad.append((torch.sign(a.grad).item())*a.grad.item())\n",
    "            f0_grad.append(f0.grad.item())\n",
    "            #f0_grad.append((torch.sign(f0.grad).item())*f0.grad.item())\n",
    "            f1_grad.append(f1.grad.item())\n",
    "            #f1_grad.append((torch.sign(f1.grad).item())*f1.grad.item())\n",
    "            t0_grad.append(t0.grad.item())\n",
    "            #t0_grad.append((torch.sign(t0.grad).item())*t0.grad.item())\n",
    "            t1_grad.append(t1.grad.item())\n",
    "            #t1_grad.append((torch.sign(t1.grad).item())*t1.grad.item())\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            with torch.no_grad():\n",
    "                a -= learning_rate * a.grad\n",
    "                f0 -= learning_rate * f0.grad\n",
    "                f1 -= learning_rate * f1.grad\n",
    "                t0 -= learning_rate * t0.grad\n",
    "                t1 -= learning_rate * t1.grad\n",
    "\n",
    "            # Zero gradients for the next iteration\n",
    "            a.grad.zero_()\n",
    "            f0.grad.zero_()\n",
    "            f1.grad.zero_()\n",
    "            t0.grad.zero_()\n",
    "            t1.grad.zero_()\n",
    "\n",
    "        # Store the final solution and its loss\n",
    "        solutions.append((ip, loss.item()))\n",
    "\n",
    "        # after the training loop\n",
    "        plt.figure()\n",
    "        plt.plot(loss_history, label='Loss')\n",
    "        plt.plot(a_grad, label='a_grad')\n",
    "        plt.plot(f0_grad, label='f0_grad')\n",
    "        plt.plot(f1_grad, label='f1_grad')\n",
    "        plt.plot(t0_grad, label='t0_grad')\n",
    "        plt.plot(t1_grad, label='t1_grad')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        plt.show()  \n",
    "\n",
    "    # Find the solution with the lowest loss\n",
    "    best_solution, best_loss = min(solutions, key=lambda solution: solution[1])\n",
    "    #print(best_solution)\n",
    "\n",
    "    a, f0, f1, t0, t1 = best_solution                                   \n",
    "    \n",
    "    p0 = (torch.cos(a))**2\n",
    "    p1 = (torch.sin(a))**2\n",
    "    #print('p0+p1:', p0.item()+p1.item())\n",
    "    a0 = torch.cos(t0)\n",
    "    a1 = torch.sin(t0)\n",
    "    #print(f'a0^2 + a1^2: {(a0.item())**2 + (a1.item())**2}')\n",
    "    b0 = torch.cos(t1)\n",
    "    b1 = torch.sin(t1)\n",
    "    #print(f'b0^2 + b1^2: {(b0.item())**2 + (b1.item())**2}')\n",
    "    ph0 = torch.exp(1j*f0)\n",
    "    ph1 = torch.exp(1j*f1)\n",
    "    #print(f'p0: {p0.item()}, p1: {p1.item()}, a0: {a0.item()}, a1: {a1.item()}, b0: {b0.item()}, b1: {b1.item()}, ph0: {ph0.item()}, ph1: {ph1.item()}')\n",
    "    \n",
    "    psi0n = torch.tensor([a0, ph0*a1], dtype=torch.cdouble)\n",
    "    psi1n = torch.tensor([b0, ph1*b1], dtype=torch.cdouble)\n",
    "    initial_states = Creating_states(coeff = coeff, abstract=False)     # Creating the two states with these coefficients\n",
    "    psi0 = initial_states[0]\n",
    "    psi1 = initial_states[1]    # created the states to be discriminated\n",
    "    psi0p = [cm.polar(psi0[0])[0], cm.polar(psi0[1])[0]*np.exp(1j*(cm.polar(psi0[1])[1]-cm.polar(psi0[0])[1]))]\n",
    "    psi1p = [cm.polar(psi1[0])[0], cm.polar(psi1[1])[0]*np.exp(1j*(cm.polar(psi1[1])[1]-cm.polar(psi1[0])[1]))]\n",
    "    psi0t = torch.tensor(psi0p, dtype=torch.cdouble)\n",
    "    psi1t = torch.tensor(psi1p, dtype=torch.cdouble)\n",
    "   \n",
    "    fid0 = torch.abs(torch.dot(psi0n, torch.conj(psi0t)))**2\n",
    "    fid1 = torch.abs(torch.dot(psi1n, torch.conj(psi1t)))**2\n",
    "\n",
    "    fid = [fid0.item(), fid1.item()]\n",
    "    p = [p0.item(), p1.item()]\n",
    "\n",
    "    return([fid,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = [0**.5,1**.5,1**.5,0**.5] \n",
    "priors = [.5,.5]\n",
    "trials = [100*(i+1) for i in range(100)]\n",
    "p0trials  = [priors[0] for i in trials] \n",
    "p1trials  = [priors[1] for i in trials]\n",
    "\n",
    "lr0 = 0.01\n",
    "#ip0 = [torch.tensor([val], requires_grad=True) for val in [torch.pi / 4, 0.0, 0.0, torch.pi / 2, 0.0]]\n",
    "#print(ip0)\n",
    "#ip = [torch.tensor([torch.pi / 4], requires_grad=True),torch.tensor([0.0], requires_grad=True),torch.tensor([0.0], requires_grad=True),torch.tensor([torch.pi / 2], requires_grad=True),torch.tensor([0.0], requires_grad=True)]\n",
    "\n",
    "initial_states = Creating_states(abstract=False, coeff = coeff)     # Creating the two states with these coefficients\n",
    "psi0 = initial_states[0]\n",
    "psi1 = initial_states[1]\n",
    "\n",
    "output_f = []\n",
    "output_p = []\n",
    "\n",
    "pfid  = [1 for i in trials]    # perfect fidelity of 1\n",
    "#for i,t in tqdm(enumerate(trials)):\n",
    "    #print(f'i_N: {i}')\n",
    "    #lr = update_lr(lr0,i)\n",
    "    #ip = ip0\n",
    "    #print(f'learning rate: {lr}')\n",
    "    #output_f.append(grad_fid(lr, t,coeff,priors)[0])\n",
    "    #output_p.append(grad_fid(lr, t,coeff,priors)[1])\n",
    "\n",
    "output_f = [grad_fid(lr0, i, coeff, priors)[0] for i in tqdm(trials)]\n",
    "output_p = [grad_fid(lr0, i, coeff, priors)[1] for i in tqdm(trials)]\n",
    "\n",
    "outputf = list(map(list, zip(*output_f)))\n",
    "outputp = list(map(list, zip(*output_p)))\n",
    "\n",
    "#print(outputf)\n",
    "#print(outputp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_a, (ax1a, ax2a) = plt.subplots(1,2, figsize=(10,4), sharey=True)\n",
    "ax1a.plot(trials, outputf[0], label=r'$F_0=|\\langle \\psi_0|\\psi_0^{\\,n}\\rangle|^2$')\n",
    "ax1a.plot(trials , pfid, \"--\", label=r'$F=1$')\n",
    "ax1a.set_ylim(0,1)\n",
    "ax1a.set_xlabel(' N (trials)')\n",
    "ax1a.set_ylabel(r'Fidelity $F$')\n",
    "ax1a.set_title(r'$|\\psi_0\\rangle ={}|0\\rangle+{}|1\\rangle $, $p_0={}$'.format(roundc(coeff[0],1), roundc(coeff[1],1), priors[0]))\n",
    "ax1a.legend(loc='best')\n",
    "ax2a.plot(trials, outputf[1], label=r'$F_1=|\\langle \\psi_1|\\psi_1^{\\,n}\\rangle|^2$')\n",
    "ax2a.plot(trials , pfid, \"--\", label=r'$F=1$')\n",
    "ax2a.set_ylim(0,1)\n",
    "ax2a.set_xlabel(' N (trials)')\n",
    "ax2a.set_ylabel(r'Fidelity $F$')\n",
    "ax2a.set_title(r'$|\\psi_1\\rangle ={}|0\\rangle+{}|1\\rangle $, $p_1={}$'.format(roundc(coeff[2],1), roundc(coeff[3],1), priors[1]))\n",
    "ax2a.legend(loc='best')\n",
    "\n",
    " \n",
    "fig_c, (ax1c, ax2c) = plt.subplots(1,2, figsize=(10,4), sharey=True)\n",
    "ax1c.plot(trials, outputp[0], label=r'$p_{0}$ numerical')\n",
    "ax1c.plot(trials, p0trials, '--', label=r'$p_{0}$ theoretical')\n",
    "ax2c.plot(trials, outputp[1], label=r'$p_{1}$ numerical')\n",
    "ax2c.plot(trials, p1trials, '--', label=r'$p_{1}$ theoretical')\n",
    "ax1c.set_ylim(0,1)\n",
    "ax2c.set_ylim(0,1)\n",
    "ax1c.set_xlabel(' N (trials)')\n",
    "ax2c.set_xlabel(' N (trials)')\n",
    "ax1c.set_ylabel(r'Probability')\n",
    "ax2c.set_ylabel(r'Probability')\n",
    "ax1c.legend(loc='best')\n",
    "ax2c.legend(loc='best')\n",
    "ax1c.set_title(r'$|\\psi_0\\rangle ={}|0\\rangle+{}|1\\rangle $, $p_0={}$'.format(roundc(coeff[0],1), roundc(coeff[1],1), priors[0]))\n",
    "ax2c.set_title(r'$|\\psi_1\\rangle ={}|0\\rangle+{}|1\\rangle $, $p_1={}$'.format(roundc(coeff[2],1), roundc(coeff[3],1), priors[1]))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

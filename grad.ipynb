{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as torch\n",
    "import math as m\n",
    "import sympy as sym\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import cmath as cm\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0673], requires_grad=True) tensor([2.8892], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create some data\n",
    "x = torch.randn(100, 1)  # 100 data points\n",
    "y = 2 * x + 3 + torch.randn(100, 1)  # true function is y = 2x + 3 # qua mettici le true probabilities\n",
    "\n",
    "# Initialize weights\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    # Forward pass: compute predictions and loss\n",
    "    y_pred = w * x + b\n",
    "    loss = (y_pred - y).pow(2).mean() #usa -log likelihood come loss or update w,b with gradient ascent (+= instead of -=)\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Zero gradients for the next iteration\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some data\n",
    "x = torch.randn(100, 5)  # 100 data points, 5 features\n",
    "y = torch.randn(100, 9)  # 100 data points, 9 outputs #or should i use other y's?\n",
    "\n",
    "# Initialize weights\n",
    "w = torch.randn(5, 9, requires_grad=True)  # 5 itorchuts, 9 outputs\n",
    "b = torch.zeros(9, requires_grad=True)  # 9 outputs\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    # Forward pass: compute predictions and loss\n",
    "    y_pred = x @ w + b  # Use matrix multiplication\n",
    "    loss = (y_pred - y).pow(2).mean() #loss = -log likelihood\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Zero gradients for the next iteration\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create some data\n",
    "x = torch.randn(100, 5)  # 100 data points, 5 features\n",
    "y = torch.randint(0, 9, (100,))  # 100 data points, 9 classes\n",
    "\n",
    "# Initialize weights\n",
    "w = torch.randn(5, 9, requires_grad=True)  # 5 itorchuts, 9 outputs\n",
    "b = torch.zeros(9, requires_grad=True)  # 9 outputs\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    # Forward pass: compute predictions and loss\n",
    "    logits = x @ w + b  # Compute logits\n",
    "    y_pred = F.log_softmax(logits, dim=1)  # Compute log probabilities\n",
    "    loss = F.nll_loss(y_pred, y)  # Compute negative log likelihood loss\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Zero gradients for the next iteration\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create some data\n",
    "x = torch.randn(100, 5)  # 100 data points, 5 features\n",
    "y = torch.randint(0, 9, (100,))  # 100 data points, 9 classes\n",
    "\n",
    "# Initialize weights\n",
    "w = torch.randn(5, 9, requires_grad=True)  # 5 itorchuts, 9 outputs\n",
    "b = torch.zeros(9, requires_grad=True)  # 9 outputs\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    # Forward pass: compute predictions and loss\n",
    "    logits = x @ w + b  # Compute logits\n",
    "    y_pred = F.softmax(logits, dim=1)  # Compute probabilities\n",
    "\n",
    "    # Compute negative log likelihood loss\n",
    "    ny = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])  # Replace with your actual values\n",
    "    loss = -torch.sum(torch.log(y_pred) * ny)\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Zero gradients for the next iteration\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Create some data\n",
    "x = torch.randn(100, 5)  # 100 data points, 5 features\n",
    "y = torch.randn(100, 1)  # 100 data points, 1 output\n",
    "\n",
    "# Initialize parameters\n",
    "w0 = torch.randn(1, requires_grad=True)\n",
    "f0 = torch.randn(1, requires_grad=True)\n",
    "t0 = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    # Forward pass: compute predictions\n",
    "    y_pred = w0 * torch.exp(x @ f0) * torch.sin(t0)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(y_pred, y)\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w0 -= learning_rate * w0.grad\n",
    "        f0 -= learning_rate * f0.grad\n",
    "        t0 -= learning_rate * t0.grad\n",
    "\n",
    "    # Zero gradients for the next iteration\n",
    "    w0.grad.zero_()\n",
    "    f0.grad.zero_()\n",
    "    t0.grad.zero_()\n",
    "\n",
    "print(w0, f0, t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create some data\n",
    "x = torch.randn(100, 5)  # 100 data points, 5 features\n",
    "y = torch.randn(100, 1)  # 100 data points, 1 output\n",
    "\n",
    "# Initialize parameters\n",
    "w0 = torch.randn(1, requires_grad=True)\n",
    "f0 = torch.randn(1, requires_grad=True)\n",
    "t0 = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    # Forward pass: compute predictions\n",
    "    y_pred = w0 * torch.exp(x @ f0) * torch.sin(t0)\n",
    "\n",
    "    # Compute negative log likelihood loss\n",
    "    residuals = y - y_pred\n",
    "    log_likelihood = -0.5 * (residuals**2 / torch.var(residuals) + torch.log(torch.var(residuals)))\n",
    "    loss = -torch.sum(log_likelihood)\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w0 -= learning_rate * w0.grad\n",
    "        f0 -= learning_rate * f0.grad\n",
    "        t0 -= learning_rate * t0.grad\n",
    "\n",
    "    # Zero gradients for the next iteration\n",
    "    w0.grad.zero_()\n",
    "    f0.grad.zero_()\n",
    "    t0.grad.zero_()\n",
    "\n",
    "print(w0, f0, t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_pred(a, f0, f1, t0,  t1):\n",
    "\n",
    "    y_pred = torch.empty(9)\n",
    "    \n",
    "    y_pred[0] = (1/6)*(((torch.cos(a[0]))**2)*torch.abs(torch.exp(1j*f0[0])*((torch.sin(t0[0]))**2) - torch.sqrt(2)*torch.cos(t0[0])*torch.sin(t0[0]))**2 + ((torch.sin(a[0]))**2)*torch.abs(torch.exp(1j*f1[0])*((torch.sin(t1[0]))**2) - torch.sqrt(2)*torch.cos(t1[0])*torch.sin(t1[0]))**2) #p1\n",
    "    y_pred[1] = (1/6)*(((torch.cos(a[1]))**2)*torch.abs(-((torch.cos(t0[1]))**2) + torch.sqrt(2)*torch.exp(1j*f0[1])*torch.cos(t0[1])*torch.sin(t0[1]))**2 + ((torch.sin(a[1]))**2)*torch.abs(-((torch.cos(t1[1]))**2) + torch.sqrt(2)*torch.exp(1j*f1[1])*torch.cos(t1[1])*torch.sin(t1[1]))**2) #p2\n",
    "    y_pred[2] = (1/6)*(((torch.cos(a[2]))**2)*torch.abs(((torch.cos(t0[2]))**2) - torch.exp(2*1j*f0[2])*((torch.sin(t0[2]))**2))**2 + ((torch.sin(a[2]))**2)*torch.abs(((torch.cos(t1[2]))**2) - torch.exp(2*1j*f1[2])*((torch.sin(t1[2]))**2))**2) #p3\n",
    "    y_pred[3] = (1/6)*(((torch.cos(a[3]))**2)*torch.abs(torch.exp(-2*1j*f0[3])*((torch.sin(t0[3]))**2) - torch.sqrt(2)*((torch.cos(a[3]))**2)*torch.exp(-1j*f0[3])*torch.cos(t0[3])*torch.sin(t0[3]))**2 + ((torch.sin(a[3]))**2)*torch.abs(torch.exp(-2*1j*f1[3])*((torch.sin(t1[3]))**2) - torch.sqrt(2)*((torch.sin(a[3]))**2)*torch.exp(-1j*f1[3])*torch.cos(t1[3])*torch.sin(t1[3]))**2) #p4\n",
    "    y_pred[4] = (1/6)*(((torch.cos(a[4]))**2)*torch.abs(-((torch.cos(t0[4]))**2) + torch.sqrt(2)*((torch.cos(a[4]))**4)*torch.exp(-1j*f0[4])*torch.cos(t0[4])*torch.sin(t0[4]))**2 + ((torch.sin(a[4]))**2)*torch.abs(-((torch.cos(t1[4]))**2) + torch.sqrt(2)*((torch.sin(a[4]))**4)*torch.exp(-1j*f1[4])*torch.cos(t1[4])*torch.sin(t1[4]))**2) #p5\n",
    "    y_pred[5] = (1/6)*(((torch.cos(a[5]))**2)*torch.abs(((torch.cos(t0[5]))**2) - ((torch.cos(a[5]))**2)*torch.exp(2*1j*f0[5])*((torch.sin(t0[5]))**2))**2 + ((torch.sin(a[5]))**2)*torch.abs(((torch.cos(t1[5]))**2) - ((torch.sin(a[5]))**2)*torch.exp(2j*f1[5])*((torch.sin(t1[5]))**2))**2) #p6\n",
    "    y_pred[6] = (1/6)*(((torch.cos(a[6]))**2)*torch.abs(torch.exp(-2*1j*f0[6])*((torch.sin(t0[6]))**2)*((torch.cos(a[6]))**2) - torch.sqrt(2)*torch.exp(-1j*f0[6])*torch.cos(t0[6])*torch.sin(t0[6]))**2 + ((torch.sin(a[6]))**2)*torch.abs(torch.exp(-2*1j*f1[6])*((torch.sin(t1[6]))**2)*((torch.sin(a[6]))**2) - torch.sqrt(2)*torch.exp(-1j*f1[6])*torch.cos(t1[6])*torch.sin(t1[6]))**2) #p7\n",
    "    y_pred[7] = (1/6)*(((torch.cos(a[7]))**2)*torch.abs(-((torch.cos(t0[7]))**2) + torch.sqrt(2)*((torch.cos(a[7]))**2)*torch.exp(-1j*f0[7])*torch.cos(t0[7])*torch.sin(t0[7]))**2 + ((torch.sin(a[7]))**2)*torch.abs(-((torch.cos(t1[7]))**2)*((torch.sin(a[7]))**2) + torch.sqrt(2)*((torch.sin(a[7]))**2)*torch.exp(-1j*f1[7])*torch.cos(t1[7])*torch.sin(t1[7]))**2) #p8\n",
    "    y_pred[8] = (1/6)*(((torch.cos(a[8]))**2)*torch.abs(((torch.cos(t0[8]))**2) - ((torch.cos(a[8]))**4)*torch.exp(-2*1j*f0[8])*((torch.sin(t0[8]))**2))**2 + ((torch.sin(a[8]))**2)*torch.abs(((torch.cos(t1[8]))**2) - ((torch.sin(a[8]))**4)*torch.exp(-2*1j*f1[8])*((torch.sin(t1[8]))**2))**2) #p9\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(ny, y_pred):\n",
    "    log_likelyhood = -torch.sum(ny * torch.log(y_pred))\n",
    "    #loss = -torch.sum(torch.log(weighted_likelihood))\n",
    "    return log_likelyhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Nepochs = 100\n",
    "\n",
    "# Create some data\n",
    "#x = torch.randn(100, 5)  # 100 data points, 5 features\n",
    "#y = torch.randn(Nepochs, 9)  # 100 data points, 9 outputs\n",
    "y_pred = torch.empty(Nepochs, 9)  # 100 data points, 9 outputs\n",
    "\n",
    "# Initialize parameters\n",
    "a = torch.randn(9, requires_grad=True)\n",
    "f0 = torch.randn(9, requires_grad=True)\n",
    "f1 = torch.randn(9, requires_grad=True)\n",
    "t0 = torch.randn(9, requires_grad=True)\n",
    "t1 = torch.randn(9, requires_grad=True)\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.01 #0.01\n",
    "\n",
    "# Set power n\n",
    "ny = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])  # Replace with your actual value\n",
    "\n",
    "# Train for 100 epochs\n",
    "for i in range(Nepochs):\n",
    "    # Forward pass: compute predictions #no mistakes here hopefully :(\n",
    "    \n",
    "    \n",
    "    # Compute negative log likelihood loss\n",
    "    residuals = y - y_pred\n",
    "    likelihood = torch.exp(-0.5 * (residuals**2 / torch.var(residuals)))\n",
    "    weighted_likelihood = y.pow(ny) * likelihood\n",
    "    loss = -torch.sum(torch.log(weighted_likelihood))\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        f0 -= learning_rate * f0.grad\n",
    "        f1 -= learning_rate * f1.grad\n",
    "        t0 -= learning_rate * t0.grad\n",
    "        t1 -= learning_rate * t1.grad\n",
    "\n",
    "    # Zero gradients for the next iteration\n",
    "    a.grad.zero_()\n",
    "    f0.grad.zero_()\n",
    "    f1.grad.zero_()\n",
    "    t0.grad.zero_()\n",
    "    t1.grad.zero_()\n",
    "\n",
    "print(a, f0, f1, t0, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
